apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: docker-build-cluster
spec:
  inputs:
    params:
      - name: task-pvc
        description: the task pvc - this is the volume where the files (Dockerfile etc..) are expected to be
      - name: ibmcloudApi
        description: the ibmcloud api
        default: https://cloud.ibm.com
      - name: continuous-delivery-context-secret
        description: name of the configmap containing the continuous delivery pipeline context secrets
        default: cd-secret
      - name: resourceGroup
        description: target resource group (name or id) for the ibmcloud login operation
        default: ''
      - name: clusterRegion
        description: (optional) the ibmcloud region hosting the cluster (if none is found it will default to the toolchain region)
        default: ''
      - name: clusterNamespace
        description: (optional) the kubernetes cluster namespace where the docker engine is hosted/deployed
        default: 'build'
    resources:
      # Will be set as optional when tekton version support it
      - name: cluster
        type: cluster
  steps:
    - name: docker-cli
      image: alpinelinux/docker-cli
      command: ["/bin/sh", "-c"]
      args:
        - |
          set -e -o pipefail
          export DOCKER_HOST='tcp://localhost:2375'
          docker version
          docker images
      volumeMounts:
        - mountPath: /artifacts
          name: task-volume
        - mountPath: /cd-config
          name: cd-config-volume
        - mountPath: /cluster/build/docker
          name: sidecar-volume          
  sidecars:
    - name: setup-docker-build-cluster
      image: ibmcom/pipeline-base-image
      env:
        - name: IBM_CLOUD_API_KEY
          valueFrom:
            secretKeyRef:
              name: $(inputs.params.continuous-delivery-context-secret)
              key: API_KEY
        - name: IBMCLOUD_TARGET_RESOURCE_GROUP
          value: $(inputs.params.resourceGroup)
        - name: IBMCLOUD_TARGET_REGION
          value: $(inputs.params.clusterRegion)
        - name: BUILD_CLUSTER_NAMESPACE
          value: $(inputs.params.clusterNamespace)
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -e -o pipefail
          ##########################################################################
          # Setting HOME explicitly to have ibmcloud plugins available
          # doing the export rather than env definition is a workaround
          # until https://github.com/tektoncd/pipeline/issues/1836 is fixed
          export HOME="/root"
          ##########################################################################

          # if target region is not specified, used the toolchain region
          if [ -z "$IBMCLOUD_TARGET_REGION" ]; then
            IBMCLOUD_TARGET_REGION=$(jq -r '.region_id' /cd-config/toolchain.json | awk -F: '{print $3}')
          fi

          # if target region is in the 'ibm:yp:<region>' format just keep the region part
          REGION_SUBSET=$(echo "$IBMCLOUD_TARGET_REGION" | awk -F ':' '{print $3;}')
          if [ -z "$REGION_SUBSET" ]; then
            echo "IBM Cloud Target Region is $IBMCLOUD_TARGET_REGION"
          else
            export IBMCLOUD_TARGET_REGION=$REGION_SUBSET
            echo "IBM Cloud Target Region is $IBMCLOUD_TARGET_REGION. export IBMCLOUD_TARGET_REGION=$REGION_SUBSET done"
          fi

          echo "Logging in to build cluster account..."
          ibmcloud config --check-version false
          ibmcloud login --apikey "$IBM_CLOUD_API_KEY" -r "$IBMCLOUD_TARGET_REGION"

          if [ -z "$IBMCLOUD_TARGET_RESOURCE_GROUP" ]; then
            echo "Using default resource group" 
          else
            ibmcloud target -g "$IBMCLOUD_TARGET_RESOURCE_GROUP"
          fi

          echo "Cluster list:"
          ibmcloud ks clusters

          echo "Running ibmcloud ks cluster config --cluster "$(inputs.resources.cluster.name)" --export -s"
          CLUSTER_CONFIG_COMMAND=$(ibmcloud ks cluster config --cluster "$(inputs.resources.cluster.name)" --export -s)
          echo "$CLUSTER_CONFIG_COMMAND"
          eval $CLUSTER_CONFIG_COMMAND

          echo "Checking cluster namespace $BUILD_CLUSTER_NAMESPACE"
          if ! kubectl get namespace "$BUILD_CLUSTER_NAMESPACE"; then
            kubectl create namespace "$BUILD_CLUSTER_NAMESPACE"
          fi

          # Ensure there is a Docker server on the build cluster
          if ! kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" rollout status -w deployment/docker; then
            echo "Installing Docker Server into build cluster..."
            kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" run docker --image=docker:18.09.2-dind --overrides='{ "apiVersion": "apps/v1", "spec": { "template": { "spec": {"containers": [ { "name": "docker", "image": "docker:18.09.2-dind", "securityContext": { "privileged": true } } ] } } } }'
            kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" rollout status -w deployment/docker
          fi

          # Use port-forward to make the pod/port locally accessible
          # Be sure to use a running POD (not an evicted one)
          kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" get pods 
          kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" port-forward $(kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" get pods | grep docker | grep -i running | awk '{print $1;}') 2375:2375

          # kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" port-forward $(kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" get pods | grep docker | grep -i running | awk '{print $1;}') 2375:2375 > /dev/null 2>&1 &

          # sudo apt-get install -y netcat
          
          # which nc

          # while ! nc -z localhost 2375; do
          #   echo "sleeping 3s"
          #   sleep 3
          # done

          # echo "build cluster seems up & running and port-forward is ok" | tee /cluster/build/docker/up
          # ls -l /cluster/build/docker
          # # Now time to keep this sidecar container up & running
          # # sleep infinity
          # trap : TERM INT; sleep infinity & wait
      volumeMounts:
        - mountPath: /cd-config
          name: cd-config-volume
        - mountPath: /cluster/build/docker
          name: sidecar-volume          
      # Wait for the port foward to be available
      readinessProbe:
        tcpSocket:
          port: 2375
        # exec:
        #   command: ["ls", "/cluster/build/docker/up"]
        initialDelaySeconds: 3
        periodSeconds: 3
        failureThreshold: 10
  volumes:
    - name: task-volume
      persistentVolumeClaim:
        claimName: $(inputs.params.task-pvc)
    - name: sidecar-volume
      emptyDir: {}
    - name: cd-config-volume
      configMap:
        name: toolchain
        items:
        - key: toolchain.json
          path: toolchain.json
